<p>Hexapod robot with Inverse Kinematics-based locomotion, self-levelling, and remote control over TCP/IP.</p>
<p><video controls><source src="/videos/hexapod.mp4" type="video/mp4; codecs=av01.0.09M.10.0.110.09.18.09.0"></video></p>
<blockquote>
<p>The finished, working hexapod</p>
</blockquote>
<h2>Project writeup</h2>
<p>I&#39;m taking Software Engineering in high school, and for our third Software Engineering assessment task this year we were tasked to build a <strong>hexapod robot</strong> from a kit which we would program to meet requirements that we set as a group. We set the following:</p>
<ul>
<li>Movement control and camera feed over Wi-Fi (or possibly the internet)</li>
<li>Self-levelling</li>
<li>Carrying objects</li>
<li>Climbing sloped surfaces</li>
<li>Activating an LED headlight in the dark</li>
</ul>
<p>The kit included some demo python code which allowed for remote control of the robot over HTTP, but it had some limitations that we would have to improve upon for our project. The gait was slow, the UI only allowed for one action at a time, and the live video feed had high latency and low framerate.
After browsing through the provided code I came to the conclusion that it would be necessary use the provided Servo and IMU modules but write my own control system and <strong>Inverse Kinematics</strong> walking algorithm.
This turned out to be a VERY large undertaking, and I worked many a late night over the course of the 6 weeks allotted for development. My first idea was to create a model of the robot in Blender, and to use Blender&#39;s python scripting to send commands to the servos. Whilst the IK worked really well, the Blender package was too heavy to run on the Raspberry Pi that the robot was based off, sending this idea into the bin.
<video controls><source src="/videos/alfred-blender.mp4" type="video/mp4; codecs=av01.0.09M.10.0.110.09.18.09.0"></video></p>
<blockquote>
<p>Hexapod model in Blender with IK</p>
</blockquote>
<p>I attempted to implement <strong>CCDIK</strong> in python next. CCD is a popular generalised method for calculating IK, but I was finding it difficult to implement, primarily because I needed to first write the <strong>Forward Kinematics</strong> algorithm to build on top of. This involved choosing data structures to represent <strong>an armature as chains of bones</strong>, converting between <strong>local and global coordinate and rotation systems</strong>, and doing this for 6 legs at different positions. As you can imagine this quickly became very complicated, and combining this with the complex iteration of the CCD algorithm lead me to dead end after dead end, with no easy way to track down where the source of error was.
After a lot more research, I finally came across an article describing how to derive the simple <strong>trigonometric equations</strong> that define the relationship between end effector and servo angles on hexapod legs specifically, meaning I could use the exact solutions rather than the close approximation given by CCD, which would also be significantly easier to implement. I also requested a review of my armature/forward kinematics code from a friend, who was able to spot some of the last remaining bugs so that I could know any future errors came from the IK calculations. 
<video controls><source src="/videos/hexapodfails.mp4" type="video/mp4; codecs=av01.0.09M.10.0.110.09.18.09.0"></video></p>
<blockquote>
<p>Progress of my IK implementation, using my Godot 3d visualisation to debug.</p>
</blockquote>
<p>After these breakthroughs it was an intense sprint before the deadline with a lot still to go.</p>
<ul>
<li><p>I built <strong>walking gaits</strong> by moving each foot along a quadratic path with a flat bottom, staggering each leg based on one of three selectable patterns which each have tradeoffs between speed, efficiency and balance.</p>
</li>
<li><p>The control input is given by a video game controller, as they are readily available and work well for controlling the <strong>three movement modes</strong>:</p>
<ul>
<li>Walk with turning radius or turn on the spot</li>
<li>Strafe</li>
<li>Look around (head only)</li>
</ul>
</li>
<li><p><strong>Self-levelling</strong> was achieved by tuning a <strong>PID</strong> control algorithm to read rotation angles given by an IMU, and rotate the <strong>root bone</strong> of the armature in the opposite direction. This essentially tells each leg which way is the new up, and the Inverse Kinematics algorithm does the rest of the work.
<img src="/images/alfred-client-ui.png" alt="alfred-client-ui.png" title="null" /></p>
<blockquote>
<p>The client GUI</p>
</blockquote>
</li>
<li><p>The client is developed in the <strong>Godot engine</strong>, chosen for built-in game controller support, rapid UI development, and my familiarity. </p>
<ul>
<li>It communicates with the hexapod over <strong>websockets</strong> for low latency control.</li>
<li>It displays the live Inverse Kinematics state in the bottom left, which I developed very early on to utilise Godot&#39;s 3d capabilities to <strong>visualise</strong> the IK state for debugging. </li>
<li>It allows for live adjustments of the robot&#39;s <strong>stance and gait</strong>. </li>
<li>Finally, it displays a live feed from the camera at a <strong>high framerate</strong> and low latency over <strong>webrtc</strong>, which allows for smooth and responsive operation of the hexapod.</li>
</ul>
</li>
<li><p>Finally, one of my teammates built an LED headlamp for the robot, which I programmed to turn on and off based on a ambient light threshold using an ESP32 microcontroller strapped to the side (at the time of writing there are some firmware issues reading Light Dependent Resistors on a Raspberry Pi, so I opted for a separate MCU)</p>
</li>
</ul>
